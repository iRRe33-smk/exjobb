{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of our problem.\n",
    "\n",
    "The following problem will be solved in each node of a dynamic programming problem:\n",
    "\n",
    "find n Actions such that they constitute a local nash equilibrium\n",
    "\n",
    "input = Action\n",
    "loss = stochatic salvo combat model: probabilty of winning\n",
    "\n",
    "restart the process after updating the state with each of these Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad, Function\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # global variables\n",
    "# N_Technologies = 3\n",
    "# N_Capabilities = 6\n",
    "# Horizon = 5\n",
    "\n",
    "# # Used in TRL calculations\n",
    "# I = 25\n",
    "# D = 0\n",
    "# CAPABILITYMATRIX = torch.rand(N_Technologies,N_Capabilities,2) # assuming differnt conversion for each of the players, informed by specific scenario \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #helper functions \n",
    "\n",
    "# def Update_State(State,Action):\n",
    "    \n",
    "#     #UpdateValue = randomness(Action) #implement randomness\n",
    "#     UpdateValue = Action\n",
    "    \n",
    "#     return torch.add(State,UpdateValue)\n",
    "\n",
    "# def TechnologyReadiness(State):\n",
    "#     global D,I\n",
    "#     TRL = torch.pow(1+torch.exp(-State*(1/I)+D),-1)\n",
    "#     return TRL\n",
    "\n",
    "# def TechToCapa(State):\n",
    "#     #Capabilities = np.matmul(np.transpose(State),CAPABILITYMATRIX)\n",
    "#     TechnologyReadinessLevel = TechnologyReadiness(State)\n",
    "#     print(TechnologyReadinessLevel)\n",
    "\n",
    "    \n",
    "#     Capabilities = torch.empty((N_Capabilities,2))\n",
    "\n",
    "#     for i in range(2):\n",
    "#         Capabilities[:,i] = torch.transpose(TechnologyReadinessLevel[:,i],0,-1) @ CAPABILITYMATRIX[:,:,i]\n",
    "        \n",
    "#     return Capabilities\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #should give probabilities that [player 1, player 2] wins the battle. \n",
    "# def torchBattle(capabilities):\n",
    "#     results = torch.div(torch.sum(capabilities,dim=0) , torch.sum(capabilities))\n",
    "#     return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 137\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mHistory\n\u001b[0;32m    136\u001b[0m FullGame \u001b[39m=\u001b[39m TorchGame(N_Technologies\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m,Horizon\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,N_actions\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m hist \u001b[39m=\u001b[39m FullGame\u001b[39m.\u001b[39;49mMain()\n",
      "Cell \u001b[1;32mIn[143], line 121\u001b[0m, in \u001b[0;36mTorchGame.Main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m st,t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ\u001b[39m.\u001b[39mpop() \u001b[39m#the state which we are currently examining\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m#print(t)\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mGetActions(st) \u001b[39m# small number of nash equilibria\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m act:\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mHistory\u001b[39m.\u001b[39mappend((st,a)) \u001b[39m# adding the entering state and the exiting action to history, reward should probably also be added. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[143], line 108\u001b[0m, in \u001b[0;36mTorchGame.GetActions\u001b[1;34m(self, State)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_actions_startpoint):\n\u001b[0;32m    107\u001b[0m     init_action \u001b[39m=\u001b[39m ActionStartPoints[:,:,i]\u001b[39m#.clone().detach().requires_grad_(True)\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     NE_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mOptimizeAction(State,  init_action)\n\u001b[0;32m    109\u001b[0m     NashEquilibria\u001b[39m.\u001b[39mappend(NE_action)\n\u001b[0;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFilterActions(NashEquilibria)\n",
      "Cell \u001b[1;32mIn[143], line 72\u001b[0m, in \u001b[0;36mTorchGame.OptimizeAction\u001b[1;34m(self, State, Action)\u001b[0m\n\u001b[0;32m     70\u001b[0m act_n \u001b[39m=\u001b[39m Action\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m dA \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 72\u001b[0m \u001b[39mwhile\u001b[39;00m torch\u001b[39m.\u001b[39;49mnorm(dA) \u001b[39m>\u001b[39m eps \u001b[39mand\u001b[39;00m iteration \u001b[39m<\u001b[39m \u001b[39m50\u001b[39m:\n\u001b[0;32m     74\u001b[0m     trl \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39madd(State,act_n)\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mI)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD),\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     77\u001b[0m     trl_temp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(torch\u001b[39m.\u001b[39mtranspose(trl,\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\IsakG\\anaconda3\\envs\\exjobb\\lib\\site-packages\\torch\\functional.py:1476\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[0;32m   1473\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1474\u001b[0m         norm, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, keepdim\u001b[39m=\u001b[39mkeepdim, out\u001b[39m=\u001b[39mout, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m-> 1476\u001b[0m ndim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdim()\n\u001b[0;32m   1478\u001b[0m \u001b[39m# catch default case\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "class TorchGame():\n",
    "    def __init__(self, N_Technologies =3, N_Capabilities = 6, Horizon = 5, N_actions = 5, N_actions_startpoint = 100, I=25, D = 0) -> None:\n",
    "        torch.manual_seed(1337)\n",
    "        # global variables\n",
    "        self.N_Technologies = N_Technologies\n",
    "        self.N_Capabilities = N_Capabilities\n",
    "        self.Horizon = Horizon\n",
    "        self.N_actions_startpoint = N_actions_startpoint\n",
    "        self.N_actions = N_actions\n",
    "        \n",
    "        # Used in TRL calculations\n",
    "        self.I = I\n",
    "        self.D = D\n",
    "        \n",
    "        #\n",
    "        self.CAPABILITYMATRIX = torch.rand(N_Technologies,N_Capabilities,2) # assuming differnt conversion for each of the players, informed by specific scenario \n",
    "\n",
    "        \n",
    "        #creating the initalState\n",
    "        st = torch.rand(N_Technologies,2)\n",
    "        divisor = 0.01*torch.sum(st,0) # sum to 100\n",
    "        self.InitialState = torch.divide(st,divisor)\n",
    "        \n",
    "        self.History = []\n",
    "        self.Q = []\n",
    "    \n",
    "    def Update_State(self,State,Action):\n",
    "        #UpdateValue = randomness(Action) #implement stochasticity\n",
    "        UpdateValue = Action\n",
    "        \n",
    "        newState = torch.add(State,UpdateValue)\n",
    "        newState.requires_grad_(True)\n",
    "        \n",
    "        return newState\n",
    "\n",
    "    def TechnologyReadiness(self,State):\n",
    "        \n",
    "        TRL = torch.pow(1+torch.exp(-State*(1/self.I)+self.D),-1)\n",
    "        TRL.requires_grad_(True)\n",
    "        return TRL\n",
    "\n",
    "    def TechToCapa(self,State):\n",
    "        \n",
    "        TechnologyReadinessLevel = self.TechnologyReadiness(State)\n",
    "        \n",
    "        \n",
    "        Capabilities = torch.empty((self.N_Capabilities,2))\n",
    "\n",
    "        for i in range(2):\n",
    "            Capabilities[:,i] = torch.transpose(TechnologyReadinessLevel[:,i],0,-1) @ self.CAPABILITYMATRIX[:,:,i]\n",
    "        Capabilities.requires_grad_(True)\n",
    "\n",
    "            \n",
    "            \n",
    "        return Capabilities\n",
    "    \n",
    "    def Battle(self,Capabilities):\n",
    "        results = torch.div(torch.sum(Capabilities,dim=0) , torch.sum(Capabilities))\n",
    "        return results\n",
    "    \n",
    "    def OptimizeAction(self, State,Action): #this should use the battle function\n",
    "\n",
    "        #this is really the only place where the whole pytorch thing is required. The rest can be base python or numpy\n",
    "        eps = 1E-2\n",
    "        iteration = 0\n",
    "        \n",
    "        learningRate = 1\n",
    "        gradFlipper = torch.transpose(torch.tensor([ [1]*self.N_Technologies , [-1] * self.N_Technologies]),0,-1)\n",
    "\n",
    "        act_n = Action.clone().detach().requires_grad_(True)\n",
    "        dA = 1\n",
    "        while torch.norm(dA) > eps and iteration < 50:\n",
    "            \n",
    "            trl = torch.pow(1+torch.exp(-torch.add(State,act_n)*(1/self.I)+self.D),-1)\n",
    "            \n",
    "            \n",
    "            trl_temp = torch.unsqueeze(torch.transpose(trl,0,-1),1)\n",
    "            capa_temp = torch.transpose(torch.transpose(self.CAPABILITYMATRIX,2,0),1,2)\n",
    "        \n",
    "\n",
    "            \n",
    "            capabilities = torch.matmul(trl_temp,capa_temp )\n",
    "            score = torch.div(torch.sum(capabilities,dim=0) , torch.sum(capabilities))\n",
    "            \n",
    "            score.backward(torch.ones_like(score))\n",
    "            \n",
    "            # print(gradAct.is_leaf)\n",
    "            \n",
    "            dA = act_n.grad\n",
    "            act_n = torch.add(act_n + dA * gradFlipper * learningRate)\n",
    "            \n",
    "            print(f\"norm(dA) = {torch.norm(dA)}, P1 winprob = {score}\")\n",
    "            \n",
    "            iteration +=1 \n",
    "\n",
    "    \n",
    "        \n",
    "        return (Action + torch.rand_like(Action)) * .5\n",
    "        \n",
    "    def FilterActions(self, Actions): #keep optimization trajectories that converged, and filter out \"duplicates\" s.t., tol < eps\n",
    "        return Actions[:self.N_actions]\n",
    "\n",
    "    def GetActions(self,State):\n",
    "        \n",
    "        ActionStartPoints = torch.rand(self.N_Technologies,2,self.N_actions_startpoint)\n",
    "        \n",
    "        NashEquilibria = []\n",
    "        for i in range(self.N_actions_startpoint):\n",
    "            init_action = ActionStartPoints[:,:,i]#.clone().detach().requires_grad_(True)\n",
    "            NE_action = self.OptimizeAction(State,  init_action)\n",
    "            NashEquilibria.append(NE_action)\n",
    "            \n",
    "         \n",
    "        return self.FilterActions(NashEquilibria)\n",
    "    \n",
    "    def Main(self):\n",
    "        start = time.time()\n",
    "        self.Q.append((self.InitialState,0))\n",
    "        \n",
    "        while (len(self.Q) > 0 and time.time() - start < 10):\n",
    "            st,t = self.Q.pop() #the state which we are currently examining\n",
    "            #print(t)\n",
    "            act = self.GetActions(st) # small number of nash equilibria\n",
    "            for a in act:\n",
    "                self.History.append((st,a)) # adding the entering state and the exiting action to history, reward should probably also be added. \n",
    "                                          \n",
    "                \n",
    "                st_new = self.Update_State(st,a) #the resulting states of traversing along the nash equilibrium\n",
    "                if t+1 < self.Horizon:\n",
    "                    self.Q.append((st_new,t+1))\n",
    "                    \n",
    "        return self.History\n",
    "                \n",
    "             \n",
    "\n",
    "            \n",
    "           \n",
    "FullGame = TorchGame(N_Technologies=21,Horizon=4,N_actions=3)\n",
    "hist = FullGame.Main()\n",
    "#print(len(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randn(2, 1, 21)\n",
    "tensor2 = torch.randn(2, 21, 6)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.2033, 3.6669, 1.9358, 2.8554, 1.4877, 3.2222, 1.8596, 2.2637, 2.8329,\n",
      "        2.5007])\n",
      "tensor([[44.0411],\n",
      "        [51.8836],\n",
      "        [27.3188],\n",
      "        [43.0039],\n",
      "        [24.6529],\n",
      "        [51.8322],\n",
      "        [26.3758],\n",
      "        [30.0112],\n",
      "        [40.4161],\n",
      "        [39.1854]])\n"
     ]
    }
   ],
   "source": [
    "a = 5\n",
    "b = 10\n",
    "#c = 12\n",
    "\n",
    "A  = torch.rand(b,a)\n",
    "x = torch.rand(b,1).requires_grad_()\n",
    "z = torch.rand(a,1)\n",
    "\n",
    "y = torch.exp(torch.transpose(A,0,-1) @ x) * z\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "\n",
    "print(torch.sum(A,1))\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1237, 0.6139],\n",
       "         [0.8393, 0.1400],\n",
       "         [0.4973, 0.1789],\n",
       "         [0.5344, 0.3982],\n",
       "         [0.5101, 0.1695],\n",
       "         [0.3680, 0.7682]],\n",
       "\n",
       "        [[0.1063, 0.7549],\n",
       "         [0.1615, 0.1600],\n",
       "         [0.7457, 0.5889],\n",
       "         [0.1432, 0.8644],\n",
       "         [0.6009, 0.1288],\n",
       "         [0.3015, 0.7617]],\n",
       "\n",
       "        [[0.7241, 0.2301],\n",
       "         [0.2023, 0.6491],\n",
       "         [0.7809, 0.7176],\n",
       "         [0.6532, 0.9485],\n",
       "         [0.8844, 0.3448],\n",
       "         [0.3793, 0.4971]],\n",
       "\n",
       "        [[0.7479, 0.9166],\n",
       "         [0.5431, 0.7336],\n",
       "         [0.1709, 0.5611],\n",
       "         [0.8779, 0.1661],\n",
       "         [0.6270, 0.8584],\n",
       "         [0.3914, 0.0303]],\n",
       "\n",
       "        [[0.6999, 0.1528],\n",
       "         [0.8424, 0.7754],\n",
       "         [0.0467, 0.5553],\n",
       "         [0.4903, 0.9525],\n",
       "         [0.8764, 0.2972],\n",
       "         [0.0605, 0.6580]],\n",
       "\n",
       "        [[0.8613, 0.2906],\n",
       "         [0.2896, 0.3111],\n",
       "         [0.5234, 0.8249],\n",
       "         [0.5707, 0.9190],\n",
       "         [0.8812, 0.2342],\n",
       "         [0.0894, 0.3608]],\n",
       "\n",
       "        [[0.8135, 0.6592],\n",
       "         [0.6161, 0.2744],\n",
       "         [0.6736, 0.9142],\n",
       "         [0.2470, 0.3262],\n",
       "         [0.8435, 0.6611],\n",
       "         [0.1178, 0.9660]],\n",
       "\n",
       "        [[0.9122, 0.0158],\n",
       "         [0.2562, 0.0505],\n",
       "         [0.0702, 0.0212],\n",
       "         [0.9169, 0.1277],\n",
       "         [0.8934, 0.6349],\n",
       "         [0.2313, 0.9601]],\n",
       "\n",
       "        [[0.2379, 0.2903],\n",
       "         [0.9272, 0.2622],\n",
       "         [0.2148, 0.3462],\n",
       "         [0.6976, 0.7260],\n",
       "         [0.3926, 0.0245],\n",
       "         [0.9056, 0.9351]],\n",
       "\n",
       "        [[0.7881, 0.8728],\n",
       "         [0.5648, 0.4636],\n",
       "         [0.8821, 0.4955],\n",
       "         [0.5048, 0.5774],\n",
       "         [0.8978, 0.8968],\n",
       "         [0.8162, 0.3740]],\n",
       "\n",
       "        [[0.0105, 0.3969],\n",
       "         [0.0866, 0.7371],\n",
       "         [0.9301, 0.1663],\n",
       "         [0.8891, 0.7836],\n",
       "         [0.4362, 0.7967],\n",
       "         [0.4656, 0.6990]],\n",
       "\n",
       "        [[0.9342, 0.0847],\n",
       "         [0.4171, 0.5505],\n",
       "         [0.7781, 0.8617],\n",
       "         [0.4792, 0.7463],\n",
       "         [0.2180, 0.3566],\n",
       "         [0.0732, 0.4625]],\n",
       "\n",
       "        [[0.6475, 0.2643],\n",
       "         [0.4530, 0.7426],\n",
       "         [0.2983, 0.1471],\n",
       "         [0.5139, 0.0310],\n",
       "         [0.3862, 0.0882],\n",
       "         [0.4888, 0.0155]],\n",
       "\n",
       "        [[0.6307, 0.0039],\n",
       "         [0.2125, 0.7522],\n",
       "         [0.1916, 0.3486],\n",
       "         [0.6444, 0.9165],\n",
       "         [0.0505, 0.1053],\n",
       "         [0.7412, 0.6409]],\n",
       "\n",
       "        [[0.6200, 0.1536],\n",
       "         [0.0674, 0.0843],\n",
       "         [0.6069, 0.3610],\n",
       "         [0.1542, 0.1595],\n",
       "         [0.9140, 0.2324],\n",
       "         [0.6002, 0.1420]],\n",
       "\n",
       "        [[0.5514, 0.6387],\n",
       "         [0.3215, 0.8997],\n",
       "         [0.9562, 0.4675],\n",
       "         [0.5402, 0.6564],\n",
       "         [0.8608, 0.2481],\n",
       "         [0.2567, 0.1437]],\n",
       "\n",
       "        [[0.4700, 0.0498],\n",
       "         [0.4189, 0.7768],\n",
       "         [0.4646, 0.3749],\n",
       "         [0.5114, 0.8813],\n",
       "         [0.8696, 0.7663],\n",
       "         [0.4875, 0.3600]],\n",
       "\n",
       "        [[0.0821, 0.4991],\n",
       "         [0.1288, 0.3713],\n",
       "         [0.8614, 0.5097],\n",
       "         [0.8213, 0.5746],\n",
       "         [0.8203, 0.9726],\n",
       "         [0.2005, 0.9824]],\n",
       "\n",
       "        [[0.0794, 0.9558],\n",
       "         [0.4088, 0.1143],\n",
       "         [0.0049, 0.3409],\n",
       "         [0.5754, 0.4265],\n",
       "         [0.7581, 0.1678],\n",
       "         [0.1271, 0.5380]],\n",
       "\n",
       "        [[0.8681, 0.4712],\n",
       "         [0.1369, 0.3488],\n",
       "         [0.7596, 0.8446],\n",
       "         [0.2638, 0.6012],\n",
       "         [0.6396, 0.5835],\n",
       "         [0.1865, 0.1988]],\n",
       "\n",
       "        [[0.8819, 0.1465],\n",
       "         [0.3369, 0.5734],\n",
       "         [0.9691, 0.6065],\n",
       "         [0.0383, 0.7335],\n",
       "         [0.5836, 0.3981],\n",
       "         [0.7408, 0.9933]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAPABILITYMATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000232918E9EB0>\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([0.5368, 0.4632], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "N_Technologies = 21\n",
    "N_Capabilities = 6\n",
    "I = 25\n",
    "D = 0\n",
    "\n",
    "CAPABILITYMATRIX = torch.rand(N_Technologies,N_Capabilities,2)\n",
    "State = torch.rand(N_Technologies,2)\n",
    "Action = torch.rand(N_Technologies,2)\n",
    "\n",
    "\n",
    "Stat = State#torch.tensor(State)\n",
    "\n",
    "     \n",
    "gradAct = Action.clone().detach().requires_grad_(True)\n",
    "\n",
    "Stat = torch.add(Stat,gradAct)\n",
    "trl = torch.pow(1+torch.exp(-Stat*(1/I)+D),-1)\n",
    "\n",
    "print(trl.grad_fn)\n",
    "trl_temp = torch.unsqueeze(torch.transpose(trl,0,-1),1)\n",
    "capa_temp = torch.transpose(torch.transpose(CAPABILITYMATRIX,2,0),1,2)\n",
    "\n",
    "\n",
    "\n",
    "capabilities = torch.squeeze(torch.matmul(trl_temp,capa_temp ))\n",
    "score = torch.sum(capabilities,dim=1) / torch.sum(capabilities)\n",
    "\n",
    "\n",
    "\n",
    "score.backward(torch.ones_like(score))\n",
    "\n",
    "# print(gradAct.is_leaf)\n",
    "\n",
    "dA = gradAct.grad        \n",
    "print(dA)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capabilities.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            gradFlipper = torch.transpose(torch.tensor([ [1]*N_Technologies , [-1] * N_Technologies]),0,-1)\n",
    "            gradFlipper.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #simple test Case \n",
    "\n",
    "# State_0 = torch.rand(N_Technologies,2)\n",
    "# divisor = 0.01*torch.sum(State_0,0) # sum to 100\n",
    "# State_0 = torch.divide(State_0,divisor)\n",
    "\n",
    "\n",
    "# Action_0 = torch.rand(N_Technologies,2)\n",
    "# divisor = 0.2*torch.sum(Action_0,0) # sum to 5\n",
    "# Action_0 = torch.divide(Action_0, divisor)\n",
    "\n",
    "# print(State_0)\n",
    "\n",
    "# State_1 = Update_State(State_0,Action_0)\n",
    "# print(State_1)\n",
    "\n",
    "# Capabilities_1 = TechToCapa(State_1)\n",
    "# print(Capabilities_1)\n",
    "\n",
    "# results = torchBattle(Capabilities_1)\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# #import matplotlib.pyplot as plt \n",
    "\n",
    "# nRange = range(4,200,20)\n",
    "# results = np.zeros((len(nRange),2))\n",
    "\n",
    "# for i,n in enumerate(nRange):\n",
    "#     bigMatrix = torch.rand(n,n).cuda()\n",
    "\n",
    "#     start = time.time()\n",
    "\n",
    "#     torch.linalg.eig(bigMatrix)\n",
    "#     end = time.time()\n",
    "\n",
    "#     gpuTime = end-start\n",
    "\n",
    "#     start = time.time()\n",
    "#     for _ in range(50):\n",
    "\n",
    "#         torch.linalg.eig(bigMatrix)\n",
    "#     end = time.time()\n",
    "#     cpuTime = end-start\n",
    "    \n",
    "#     results[i,:] = [gpuTime,cpuTime]\n",
    "\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# time.sleep(1)\n",
    "# end = time.time()\n",
    "# total = end-start\n",
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6634e-01, 2.0182e-02],\n",
      "        [5.2397e-01, 1.0925e-01],\n",
      "        [4.5572e-01, 5.3390e-02],\n",
      "        [1.5031e-01, 5.1849e-04],\n",
      "        [1.2115e-01, 2.1760e-04]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3669, 0.5352, 0.4588, 0.1503, 0.1212])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random points in hypersphere\n",
    "\n",
    "def randomPointsInShere(nPoints,nDim,rad):\n",
    "    #https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates\n",
    "    \n",
    "    params = torch.rand(size = (nPoints,nDim+1))\n",
    "    \n",
    "    r = params[:,-1] * rad\n",
    "    X = (r * torch.eye(nPoints)) @ torch.ones(nPoints,nDim) \n",
    "\n",
    "    for i in range(nDim-1):\n",
    "        X[:,i] *= torch.cos(X[:,i])\n",
    "        #print(f\"c{i}\")\n",
    "        for j in range(i):\n",
    "            X[:,i] *= torch.sin(X[:,j])\n",
    "            #print(f\"s{j}\")\n",
    "    \n",
    "    for j in range(nDim):\n",
    "            X[:,nDim-1] *= torch.sin(X[:,j])\n",
    "            #print(f\"s{j}\")\n",
    "    \n",
    "    return X \n",
    "\n",
    "\n",
    "    \n",
    "nPoints = 5\n",
    "nDim = 2\n",
    "\n",
    "X = randomPointsInShere(nPoints,nDim, 1)\n",
    "print(X)\n",
    "\n",
    "torch.norm(X,p=2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PlayerA_y  PlayerB_y\n",
      "sen_tec              4          4\n",
      "col_sys_alg          1          1\n",
      "tec_mob              5          5\n",
      "cont_alg             3          3\n",
      "loc_map              2          2\n",
      "sen_fus              3          3\n",
      "ai_ml                5          5\n",
      "edg_com              3          3\n",
      "com_net              5          5\n",
      "ene_mgm              2          2\n",
      "sim_mod              1          1\n",
      "          sen_tec  col_sys_alg  tec_mob  cont_alg  loc_map  sen_fus  ai_ml  \\\n",
      "A,B             0            0        0         0        0        0      0   \n",
      "Phi, Psi        9            3        3         1        9        9      3   \n",
      "n_a,n_b         0            3        1         1        1        0      3   \n",
      "p_a,p_b         3            9        9         9        3        1      1   \n",
      "n_y,n_z         9            1        3         3        1        3      3   \n",
      "p_y,p_z         9            9        9         3        3        9      9   \n",
      "u,v             0            0        0         0        0        0      0   \n",
      "w,x             0            0        0         0        0        0      0   \n",
      "\n",
      "          edg_com  com_net  ene_mgm  sim_mod  \n",
      "A,B             0        0        0        0  \n",
      "Phi, Psi        3        3        3        1  \n",
      "n_a,n_b         1        1        1        1  \n",
      "p_a,p_b         3        3        1        1  \n",
      "n_y,n_z         3        1        1        9  \n",
      "p_y,p_z         3        1        1        1  \n",
      "u,v             0        0        0        0  \n",
      "w,x             0        0        0        0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_stat = pd.read_excel(\"config_files/State_Conversion.xlsx\", sheet_name=\"StartingState\",header=0, index_col=0)\n",
    "print(df_stat)\n",
    "\n",
    "df_capaMat = pd.read_excel(\"config_files/State_Conversion.xlsx\", sheet_name=\"ConversionMatrix\",header=0, index_col=0)\n",
    "print(df_capaMat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aff9d80262916b0caa4829430b38b438ed5728c44721b4c84a6f617b453f5eb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
