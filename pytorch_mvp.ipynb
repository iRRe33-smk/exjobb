{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of our problem.\n",
    "\n",
    "The following problem will be solved in each node of a dynamic programming problem:\n",
    "\n",
    "find n Actions such that they constitute a local nash equilibrium\n",
    "\n",
    "input = Action\n",
    "loss = stochatic salvo combat model: probabilty of winning\n",
    "\n",
    "restart the process after updating the state with each of these Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "N_Technologies = 3\n",
    "N_Capabilities = 6\n",
    "Horizon = 5\n",
    "\n",
    "# Used in TRL calculations\n",
    "I = 25\n",
    "D = 0\n",
    "CAPABILITYMATRIX = torch.rand(N_Technologies,N_Capabilities,2) # assuming differnt conversion for each of the players, informed by specific scenario \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions \n",
    "\n",
    "def Update_State(State,Action):\n",
    "    \n",
    "    #UpdateValue = randomness(Action) #implement randomness\n",
    "    UpdateValue = Action\n",
    "    \n",
    "    return torch.add(State,UpdateValue)\n",
    "\n",
    "def TechnologyReadiness(State):\n",
    "    global D,I\n",
    "    TRL = torch.pow(1+torch.exp(-State*(1/I)+D),-1)\n",
    "    return TRL\n",
    "\n",
    "def TechToCapa(State):\n",
    "    #Capabilities = np.matmul(np.transpose(State),CAPABILITYMATRIX)\n",
    "    TechnologyReadinessLevel = TechnologyReadiness(State)\n",
    "    print(TechnologyReadinessLevel)\n",
    "\n",
    "    \n",
    "    Capabilities = torch.empty((N_Capabilities,2))\n",
    "\n",
    "    for i in range(2):\n",
    "        Capabilities[:,i] = torch.transpose(TechnologyReadinessLevel[:,i],0,-1) @ CAPABILITYMATRIX[:,:,i]\n",
    "        \n",
    "    return Capabilities\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should give probabilities that [player 1, player 2] wins the battle. \n",
    "def torchBattle(capabilities):\n",
    "    results = torch.div(torch.sum(capabilities,dim=0) , torch.sum(capabilities))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19607\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "class TorchGame():\n",
    "    def __init__(self, N_Technologies =3, N_Capabilities = 6, Horizon = 5, N_actions = 5, N_actions_startpoint = 100, I=25, D = 0) -> None:\n",
    "        torch.manual_seed(1337)\n",
    "        # global variables\n",
    "        self.N_Technologies = N_Technologies\n",
    "        self.N_Capabilities = N_Capabilities\n",
    "        self.Horizon = Horizon\n",
    "        self.N_actions_startpoint = N_actions_startpoint\n",
    "\n",
    "        # Used in TRL calculations\n",
    "        self.I = I\n",
    "        self.D = 0\n",
    "        self.N_actions = N_actions\n",
    "        self.CAPABILITYMATRIX = torch.rand(N_Technologies,N_Capabilities,2) # assuming differnt conversion for each of the players, informed by specific scenario \n",
    "\n",
    "        \n",
    "        #creating the initalState\n",
    "        st = torch.rand(N_Technologies,2)\n",
    "        divisor = 0.01*torch.sum(st,0) # sum to 100\n",
    "        self.InitialState = torch.divide(st,divisor)\n",
    "        \n",
    "        self.History = []\n",
    "        self.Q = []\n",
    "    \n",
    "    def Update_State(self,State,Action):\n",
    "        #UpdateValue = randomness(Action) #implement stochasticity\n",
    "        UpdateValue = Action\n",
    "        \n",
    "        return torch.add(State,UpdateValue)\n",
    "\n",
    "    def TechnologyReadiness(self,State):\n",
    "        \n",
    "        TRL = torch.pow(1+torch.exp(-State*(1/self.I)+self.D),-1)\n",
    "        \n",
    "        return TRL\n",
    "\n",
    "    def TechToCapa(self,State):\n",
    "        #Capabilities = np.matmul(np.transpose(State),CAPABILITYMATRIX)\n",
    "        TechnologyReadinessLevel = TechnologyReadiness(State)\n",
    "        print(TechnologyReadinessLevel)\n",
    "\n",
    "        \n",
    "        Capabilities = torch.empty((N_Capabilities,2))\n",
    "\n",
    "        for i in range(2):\n",
    "            Capabilities[:,i] = torch.transpose(TechnologyReadinessLevel[:,i],0,-1) @ CAPABILITYMATRIX[:,:,i]\n",
    "            \n",
    "        return Capabilities\n",
    "    \n",
    "    def Battle(self,Capabilities):\n",
    "        results = torch.div(torch.sum(Capabilities,dim=0) , torch.sum(Capabilities))\n",
    "        return results\n",
    "    \n",
    "    def OptimizeAction(self, state,Action): #this should use the battle function\n",
    "        return (Action + torch.rand_like(Action)) * .5\n",
    "        \n",
    "    def FilterActions(self, Actions): #keep optimization trajectories that converged, and filter out \"duplicates\" s.t., tol < eps\n",
    "        return Actions[:self.N_actions]\n",
    "\n",
    "    def GetActions(self,State):\n",
    "        \n",
    "        ActionStartPoints = torch.rand(self.N_Technologies,2,self.N_actions_startpoint)\n",
    "            \n",
    "        AllActions = [self.OptimizeAction(State, ActionStartPoints[:,:,i]) for i in range(self.N_actions_startpoint)] \n",
    "         \n",
    "        return self.FilterActions(AllActions)\n",
    "    \n",
    "    def Main(self):\n",
    "        start = time.time()\n",
    "        self.Q.append((self.InitialState,0))\n",
    "        \n",
    "        while (len(self.Q) > 0 and time.time() - start < 10):\n",
    "            st,t = self.Q.pop() #the state which we are currently examining\n",
    "            #print(t)\n",
    "            act = self.GetActions(st) # small number of nash equilibria\n",
    "            for a in act:\n",
    "                self.History.append((st,a)) # adding the entering state and the exiting action to history, reward should probably also be added. \n",
    "                                          \n",
    "                \n",
    "                st_new = self.Update_State(st,a) #the resulting states of traversing along the nash equilibrium\n",
    "                if t+1 < self.Horizon:\n",
    "                    self.Q.append((st_new,t+1))\n",
    "                    \n",
    "        return self.History\n",
    "                \n",
    "             \n",
    "\n",
    "            \n",
    "           \n",
    "FullGame = TorchGame(N_Technologies=21,Horizon=5,N_actions=7)\n",
    "hist = FullGame.Main()\n",
    "print(len(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5375, 50.1063],\n",
      "        [54.0613,  4.8819],\n",
      "        [40.4012, 45.0118]])\n",
      "tensor([[ 7.4430, 52.2487],\n",
      "        [55.5556,  5.0454],\n",
      "        [42.0014, 47.7059]])\n",
      "tensor([[0.5739, 0.8899],\n",
      "        [0.9022, 0.5503],\n",
      "        [0.8429, 0.8708]])\n",
      "tensor([[0.5584, 1.7318],\n",
      "        [1.0899, 1.4199],\n",
      "        [1.0759, 1.2462],\n",
      "        [1.0356, 1.3739],\n",
      "        [1.1839, 1.0753],\n",
      "        [0.6875, 1.3174]])\n",
      "tensor([0.4082, 0.5918])\n"
     ]
    }
   ],
   "source": [
    "#simple test Case \n",
    "\n",
    "State_0 = torch.rand(N_Technologies,2)\n",
    "divisor = 0.01*torch.sum(State_0,0) # sum to 100\n",
    "State_0 = torch.divide(State_0,divisor)\n",
    "\n",
    "\n",
    "Action_0 = torch.rand(N_Technologies,2)\n",
    "divisor = 0.2*torch.sum(Action_0,0) # sum to 5\n",
    "Action_0 = torch.divide(Action_0, divisor)\n",
    "\n",
    "print(State_0)\n",
    "\n",
    "State_1 = Update_State(State_0,Action_0)\n",
    "print(State_1)\n",
    "\n",
    "Capabilities_1 = TechToCapa(State_1)\n",
    "print(Capabilities_1)\n",
    "\n",
    "results = torchBattle(Capabilities_1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31498837e+00 1.40035152e-02]\n",
      " [1.99794769e-03 1.99964046e-02]\n",
      " [1.00088120e-03 2.80001163e-02]\n",
      " [2.00152397e-03 4.89976406e-02]\n",
      " [3.00145149e-03 1.20999336e-01]\n",
      " [5.00249863e-03 1.90996885e-01]\n",
      " [3.99780273e-03 2.85028458e-01]\n",
      " [8.00204277e-03 3.63998175e-01]\n",
      " [1.50015354e-02 5.90999603e-01]\n",
      " [1.59997940e-02 7.02998877e-01]]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# #import matplotlib.pyplot as plt \n",
    "\n",
    "# nRange = range(4,200,20)\n",
    "# results = np.zeros((len(nRange),2))\n",
    "\n",
    "# for i,n in enumerate(nRange):\n",
    "#     bigMatrix = torch.rand(n,n).cuda()\n",
    "\n",
    "#     start = time.time()\n",
    "\n",
    "#     torch.linalg.eig(bigMatrix)\n",
    "#     end = time.time()\n",
    "\n",
    "#     gpuTime = end-start\n",
    "\n",
    "#     start = time.time()\n",
    "#     for _ in range(50):\n",
    "\n",
    "#         torch.linalg.eig(bigMatrix)\n",
    "#     end = time.time()\n",
    "#     cpuTime = end-start\n",
    "    \n",
    "#     results[i,:] = [gpuTime,cpuTime]\n",
    "\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0065457820892334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "total = end-start\n",
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aff9d80262916b0caa4829430b38b438ed5728c44721b4c84a6f617b453f5eb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
